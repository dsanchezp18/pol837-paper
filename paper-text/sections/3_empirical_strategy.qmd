---
# Empirical Strategy
suppress-bibliography: true
---
```{r}
#| label: empirical-strategy-setup
#| include: false

# Libraries

library(dplyr)
library(modelsummary)
library(kableExtra)

# Load the data

load("data/full_df.RData")

# Define modelsummary arguments for tables

stars <- c("*" = 0.1,
           "**" = 0.05,
           "***" = 0.05)

# Do the goodness of fit mapping as a list mapping

gf_map <- list(
  list("raw" = "nobs", "clean" = "N", "fmt" = 0),
  list("raw" = "aic", "clean" = "AIC", "fmt" = 0),
  list("raw" = "rmse", "clean" = "RMSE", "fmt" = 3),
  list("raw" = "FE: canton_dpa", "clean" = "Canton fixed effects", "fmt" = 0),
  list("raw" = "FE: interview_date", "clean" = "Interview date fixed effects", "fmt" = 0)
)
```

# Empirical Approach

## Data

My data are composed of a pooled cross section of the AmericasBarometer (AB) [@ABDatasets] merged with daily CPC Global Unified temperature [@NOAADatasets] based on interview date and canton in Ecuador. The AB is a public opinion survey conducted by the Latin American Public Opinion Project (LAPOP), which has conducted biyearly survey waves in Ecuador and other countries from 2004 to 2023. I use the subscriber LAPOP datasets available through Universidad San Francisco de Quito's research affiliation with LAPOP, focusing on the eight survey waves carried out between 2008 to 2023[^1]. The surveys are based on a multi-stage national probability design, representative at the national level, except for 2021, where the survey switched to a random-digit-dialing design due to the COVID-19 pandemic.

The explained variable of interest is presidential job approval, which the AmericasBarometer measures as in a 1-5 scale in the question: \textquote{Speaking in general of the current administration, how would you rate the job performance of President [NAME]} [@ABDatasets] (p.14), where 1 represents a very good performance and 5 terrible performance. This question is similarly similarly to the classic Gallup presidential approval question, which the literature has used extensively [@berlemanneconomicdeterminantspresidential2014] and has not been found to significantly deviate from other presidential popularity measures. I dichotomize the variable following LAPOP research reports [@laytonChapterCitizenSecurity2016], where responses greater than 3 are considerar as approval for the incumbent president. 

[^1]: The 2004 and 2006 waves did not record interview dates. The eight waves took place every two years between 2008 and 2016. The 2018/19 wave took place between late 2018 and 2019. The two most recent two survey waves were carried out in 2021 and 2023.

Table 1 below displays descriptive statistics for the variables used in the empirical analysis. I collect opinion on personal economic situations and on country economic situation, also measured on a 1-5 scale, where 1 represents a very good situation and 5 a terrible situation. Political ideology is represent in a 0-10 scale, where 0 represents the "extreme left" and 10 the extreme right. I include 1-7 scales for trust in police, local government, political pride, and support for democracy where 0 represents no trust or support and 7 complete the opposite. Corruption perceptions are collected by the AB as a 1-4 scale, where 1 represents "corruption not generalized" and 4 "very generalized" [@ABDatasets] (p. 22). I dichotomize this variable taking values greater than 1 as perceiving corruption. Corruption tolerance is measured as tolerance to paying a bribe, where 0 is not justified, and 1 is justified. The empirical analysis also includes demographic controls, where labour market status includes three categories: employed, not in the labour force, and unemployed. Not being in the labour force includes retired, students, homemakers, and those not working. Education is a categorical variable for highest educational degree attained, including levels for no education, primary, secondary, and higher education (college, university or higher are lumped together). 

\begin{landscape}
\begin{footnotesize}
```{r}
#| label: descriptive-stats
#| output: asis

# Prepare relevant variables for all years

df_descriptive <- 
  df %>%
  transmute(`Presidential approval` = approves_president,
            `Daily minimum temperature (C)` = min_temperature, 
            `Daily maximum temperature (C)` = max_temperature, 
            `Daily average temperature (C)` = avg_temperature, 
            `Daily precipitation (mm)` = precipitation,
            `Female` = sex,
            `Age (years)` = age,
            `Rural status` = urban_rural,
            `Education` = education,
            `Labour market status` = labour_market,
            `Worse perception of personal economy` = personal_econ_situation,
            `Worse perception of country economy` = country_econ_situation,
            `Ideology score (0-10)` = ideology,
            `Support of democracy` = democracy_support,
            `Perception of corruption` = corruption_perception,
            `Tolerance to bribes` = corruption_tolerance,
            `Political pride score` = political_pride,
            `Trust in police score (0-7)` = confidence_police,
            `Trust in local government score (0-7)` = confidence_local_gov,
  )

# Notes

notes_descriptive <- list(
  "Note: Descriptive statistics for variables used in the empirical analysis. For categorical variables, the percent of observations in the category out of the total sample is presented. For numerical (either ordinal or continuous) variables, the mean, standard deviation, minimum and maximum are presented. For both, the number of observations and the percentage of missing values."
)

# Use datasummary() to create the table

datasummary((Education + Female + `Labour market status` + `Worse perception of personal economy` + `Worse perception of country economy` + `Perception of corruption` + `Tolerance to bribes` + All(df_descriptive)) ~ ((N) + Percent() + (`Missing (%)` = PercentMissing) + Mean + (`Std. dev.` = SD) + Min + Median + Max + Percent()),
            data = df_descriptive,
            fmt = 2,
            output = 'latex',
            booktabs = TRUE,
            title = "Descriptive statistics for the matched AB data and weather variables",
            threeparttable = TRUE,
            notes = notes_descriptive) %>% 
kable_styling(latex_options = c("hold_position"), font_size = 8)
```
\end{footnotesize}
\end{landscape}

I extract daily minimum and maximum temperature and precipitation data from the CPC Global Unified Temperature datasets [@NOAADatasets]. These datasets are prepared by the U.S. government National Oceanic and Atmospheric Administration (NOAA) and result from satellite imaging of the Earth surface. While typically daily weather data would be available from every country's meteorological authority^[In Ecuador, the relevant institution is the Instituto Nacional de Meteorología e Hidrología.], the Ecuadorian publicly available meteorological data lacks the frequency and geospatial granularity required for this type of analysis. 

The temperature data from NOAA is of lower quality than that of a typical meteorological authority, given that this data is global gridded GTS data (0.5\textdegree \texttimes 0.5\textdegree) for temperature and gauge-based for precipitation. However, I follow Quijano-Ruiz [-@quijano-ruizAssessingReliabilitySelfrated2023] and compute weighted mean minimum and maximum temperatures for each canton^[Cantons in Ecuador are the second highest level of the political-administrative division of the country, after to provinces. They are similar to municipalities.]. and day, where the weights are the surface area of each canton. Replication code for this process is publicly available in a [GitHub repository](https://github.com/laboratoriolide/ecuador-temperature-noaa). The surface area of each canton is obtained the Ecuadorian statistics authority (INEC, for its initials in Spanish) geoportal, along with the map shapefiles and political administrative divisions to match the canton names and codes to the AB data [@INECGeoportal]. I then merge this data with the AB data using the interview dates and canton codes as join identifiers. 

![Survey dates of the Americas Barometer in Ecuador, 2008-2023](/figures/interview_dates_barchart.png){#fig-interviewdates}

<!-- \begin{figure}[H] -->
<!-- \label{fig:interviewdates} -->
<!-- \centering -->
<!-- \includegraphics{/figures/interview_dates_barchart.png} -->
<!-- \caption{} -->
<!-- \end{figure} -->

@fig-interviewdates shows the distribution of respondents by interview date in the AmericasBarometer survey waves. As it can be seen, most stay in a relatively small time frame. The 2018 wave (sometimes referred as the 2018/19 wave) is the most spread out due to the survey being carried out between late 2018 and early 2019. Most waves are carried out January to April. These periods contain rich variation of temperature for Ecuador's diverse geography.

<!-- \begin{figure}[H] -->
<!-- \label{fig:temperature} -->
<!-- \centering -->
<!-- \includegraphics{/figures/ecuador_monthly_mean_temps_fig.png} -->
<!-- \caption{Mean monthly temperatures, 2008-2023} -->
<!-- \end{figure} -->

![Mean monthly temperatures, 2008-2023](/figures/ecuador_monthly_mean_temps_fig.png){#fig-temperature}

In @fig-temperature above I show mean monthly minimum and maximum temperatures from 2008 to 2023. There are no notable upward or downward trends through time, with some periods showing higher temperatures, possible due to El Niño events [@yehNinochangingclimate2009]. An important feature is that the spread between minimum and maximum temperatures is relatively stable, which will be important for the identification strategy, which I describe below. 

## Identification strategy

I exploit variation produced by a natural experiment: the transitory nature of daily temperature changes. I assume these changes are random and exogenous to variables related to political mechanisms or other variables that can affect the performance of political leaders. By making this assumption, I can definme a presidential population function as follows:

```{=tex}
\begin{equation}
\label{eqn:baseline}
 \text{approval}_{it} = \alpha + \tau_d + \theta_j + \beta \text{temp}_{jd} + \mathbb{X'}_{it} \gamma + u_{it}
\end{equation}
```
where $\text{approval}_{it}$ is presidential approval, $\tau_d$ and $\theta_j$ are vectors of interview date and canton fixed effects, $\text{temp}_{jd}$ is daily temperature, $\mathbb{X'}_{it}$ a vector of survey-wave and individual varying controls, $\gamma$ the vector of associated control coefficients and $u_{it}$ an error term. The parameter $\beta$ is the coefficient of interest, which measures the effect of temperature on presidential approval. The assumption of randomness in temperature changes implies that

\begin{equation}
\label{eqn:identification}
\text{E}[\text{temp}_{jt} \times u_{it}] = 0
\end{equation}

which allows me to estimate $\beta$ consistently. However, if temperature as measured by the CPC Global Unified Temperature datasets suffers from measurement error, $\hat{\beta}$ can suffer from attenuation bias, which makes me underestimate the true effect of temperature on presidential approval. Attenuation bias will exist if measurement error is more likely to be present in days with higher or lower temperatures, or for certain cantons. There is no reason to assume this is the case, but I address this possibility later in the paper. If measurement error is present but not correlated with the error term, then $\hat{\beta}$ will still be consistently estimated, but with less precision. 

Further, given that I only observe presidential approval in an ordinal or binary scale, I cannot directly estimate Equation 1. While it is possible to use a linear probability model, I choose to follow the literature and use logistic regression to estimate a variant of Equation 1, as follows:

```{=tex}
\begin{equation}
\label{eqn:logit}
P(\text{approval}_{it} = 1) = G(\lambda \mathbf{R}') = G(\alpha + \tau_d + \theta_j + \beta \text{temp}_{jd} + \mathbb{X'}_{it} \gamma + u_{it})
\end{equation}
```
where $P(\text{approval}_{it} = 1)$ is the probability of approving the incumbent president, $G$ is the link function, $\mathbf{R}$ is a vector of explanatory variables, which includes all variables in Equation 1, and $\lambda$ is the associated vector of coefficients. I estimate Equation 2 using the logistic function as $G$. I cluster all standard errors at the canton level, to allow for spatially clustered correlation in the error term.